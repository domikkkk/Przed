{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 4 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie algorytmu drzewa decyzyjnego ID3 dla zadania klasyfikacji. Trening i test należy przeprowadzić dla zbioru Iris. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania drzewa dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Implementacja funkcji entropii - **0.5 pkt**\n",
        "* Implementacja funkcji entropii zbioru - **0.5 pkt**\n",
        "* Implementacja funkcji information gain - **0.5 pkt**\n",
        "* Zbudowanie poprawnie działającego drzewa klasyfikacyjnego i przetestowanie go na wspomnianym wcześniej zbiorze testowym. Jeśli w liściu występuje kilka różnych klas, decyzją jest klasa większościowa. Policzenie accuracy i wypisanie parami klasy rzeczywistej i predykcji. - **4 pkt**\n",
        "* Przeprowadzenie eksperymentów dla różnych głębokości drzew i podziałów zbioru treningowego i testowego (zmiana wartości argumentu test_size oraz usunięcie random_state). W tym przypadku dla każdego eksperymentu należy wykonać kilka uruchomień programu i wypisać dla każdego uruchomienia accuracy. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from statistics import median\n",
        "\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "def entropy_func(class_count, num_samples):\n",
        "    p = class_count / num_samples\n",
        "    return -p * math.log(p)\n",
        "\n",
        "class Group:\n",
        "    def __init__(self, group_classes):\n",
        "        self.group_classes = list(group_classes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.group_classes)\n",
        "\n",
        "    def group_entropy(self):\n",
        "        value = 0\n",
        "        lenght  = len(self)\n",
        "        counter = Counter(self.group_classes)\n",
        "        for valu in counter.values():\n",
        "            value += entropy_func(valu, lenght)\n",
        "        return value\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, split_feature, split_val, child_node_a=None, child_node_b=None, val=None):\n",
        "        self.split_feature = split_feature\n",
        "        self.split_val = split_val\n",
        "        self.child_node_a = child_node_a\n",
        "        self.child_node_b = child_node_b\n",
        "        self.val = val\n",
        "\n",
        "    def predict(self, data):\n",
        "        if self.child_node_a and self.child_node_b:\n",
        "            return self.child_node_a.predict(data) if data[self.split_feature] < self.split_val else self.child_node_b.predict(data)\n",
        "        else:\n",
        "            return self.val\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self):\n",
        "        self.tree = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_split_entropy(group_a, group_b):\n",
        "        groupA = Group(group_a)\n",
        "        groupB = Group(group_b)\n",
        "        #  return groupA.group_entropy(), groupB.group_entropy()\n",
        "        return (groupA.group_entropy() * len(groupA) + groupB.group_entropy() * len(groupB)) / (len(groupA) + len(groupB))\n",
        "\n",
        "    def get_information_gain(self, parent_group, child_group_a, child_group_b):\n",
        "        parent_value = Group(parent_group).group_entropy()\n",
        "        # groupA_value, groupB_value = self.get_split_entropy(child_group_a, child_group_b)\n",
        "        # return parent_value - (len(child_group_a)/len(parent_group) * groupA_value + len(child_group_b)/len(parent_group) * groupB_value)\n",
        "        return parent_value - self.get_split_entropy(child_group_a, child_group_b)\n",
        "\n",
        "    def get_best_feature_split(self, feature_value, data, classes):\n",
        "        temp_data = [i[feature_value] for i in data]\n",
        "        split_value = 0\n",
        "        gain = 0\n",
        "        for ev_value in set(temp_data):\n",
        "            clas_a = np.array([])\n",
        "            clas_b = np.array([])\n",
        "            for value, clas in zip(temp_data, classes):\n",
        "                if value < ev_value:\n",
        "                    clas_a = np.append(clas_a, clas)\n",
        "                else:\n",
        "                    clas_b = np.append(clas_b, clas)\n",
        "            new_gain = self.get_information_gain(classes, clas_a, clas_b)\n",
        "            if new_gain > gain:\n",
        "                gain = new_gain\n",
        "                split_value = ev_value\n",
        "        return split_value, gain\n",
        "\n",
        "    def get_best_split(self, data, classes):\n",
        "        best_gain = 0\n",
        "        best_split_value = 0\n",
        "        best_feature_split = 0\n",
        "        for split_feature in [0, 1, 2, 3]:\n",
        "            split_value, gain = self.get_best_feature_split(split_feature, data, classes)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_split_value = split_value\n",
        "                best_feature_split = split_feature\n",
        "        return best_split_value, best_feature_split\n",
        "\n",
        "    def build_tree(self, data, classes, depth):\n",
        "        if len(set(classes)) == 1:\n",
        "            return Node(None, None, val=classes[0])\n",
        "        if depth == 0:\n",
        "            counter = Counter(classes)\n",
        "            val = list(counter.keys())[0]\n",
        "            return Node(None, None, val=val)\n",
        "        else:\n",
        "            ###########################################################\n",
        "             #                    mediana                             #\n",
        "            ###########################################################\n",
        "            # gain = 0\n",
        "            # feature_split = 0\n",
        "            # split_val = 0\n",
        "            # for feature in [0, 1, 2, 3]:\n",
        "            #     parent_data = [i[feature] for i in data]\n",
        "            #     temp_split_val = median(parent_data)\n",
        "            #     group_a = []\n",
        "            #     group_b = []\n",
        "            #     for val, clas in zip(parent_data, classes):\n",
        "            #         if val < temp_split_val:\n",
        "            #             group_a.append(clas)\n",
        "            #         else:\n",
        "            #             group_b.append(clas)\n",
        "            #     new_gain = self.get_information_gain(classes, group_a, group_b)\n",
        "            #     if new_gain > gain:\n",
        "            #         gain = new_gain\n",
        "            #         feature_split = feature\n",
        "            #         split_val = temp_split_val\n",
        "            split_val, feature_split = self.get_best_split(data, classes)\n",
        "            data_a = []\n",
        "            data_b = []\n",
        "            classes_a = []\n",
        "            classes_b = []\n",
        "            for line, clas in zip(data, classes):\n",
        "                if line[feature_split] < split_val:\n",
        "                    data_a.append(line)\n",
        "                    classes_a.append(clas)\n",
        "                else:\n",
        "                    data_b.append(line)\n",
        "                    classes_b.append(clas)\n",
        "            return Node(feature_split, split_val, self.build_tree(np.array(data_a), np.array(classes_a), depth-1), self.build_tree(np.array(data_b), np.array(classes_b), depth-1))\n",
        "\n",
        "    def predict(self, data):\n",
        "        return self.tree.predict(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "U033RY1_YS8x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=123)\n",
        "dc = DecisionTreeClassifier()\n",
        "dc.tree = dc.build_tree(x_train, y_train, 3)\n",
        "count = 0\n",
        "for sample, gt in zip(x_test, y_test):\n",
        "    prediction = dc.predict(sample)\n",
        "    count += prediction==gt\n",
        "print(count/len(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8666666666666667\n",
            "1.0\n",
            "0.9333333333333333\n",
            "0.9333333333333333\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 3)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n",
            "0.8\n",
            "0.9333333333333333\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 4)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n",
            "0.9333333333333333\n",
            "1.0\n",
            "0.9333333333333333\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 5)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8666666666666667\n",
            "0.9333333333333333\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 6)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9\n",
            "0.9\n",
            "0.9666666666666667\n",
            "0.9666666666666667\n",
            "0.8666666666666667\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 3)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n",
            "0.9\n",
            "0.9\n",
            "0.9\n",
            "0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 4)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.9\n",
            "1.0\n",
            "0.8333333333333334\n",
            "0.9\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 5)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9666666666666667\n",
            "0.9\n",
            "0.9\n",
            "0.9333333333333333\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "i = 5\n",
        "while i>0:  \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "    dc = DecisionTreeClassifier()\n",
        "    dc.tree = dc.build_tree(x_train, y_train, 6)\n",
        "    count = 0\n",
        "    for sample, gt in zip(x_test, y_test):\n",
        "        prediction = dc.predict(sample)\n",
        "        count += prediction==gt\n",
        "    print(count/len(x_test))\n",
        "    i -= 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d82ac94077643f57bd8e27ef7c0092ba99f1c8d5d58dd1340bf8629343aa5cca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
