{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "# Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa. \n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt** \n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        train_features = self.data_discretization(train_features)\n",
        "        class_counts = Counter(train_classes)\n",
        "        num_samples = len(train_classes)\n",
        "        for c in class_counts:\n",
        "            self.priors[c] = class_counts[c] / num_samples\n",
        "        \n",
        "        for c in self.priors:\n",
        "            self.likelihoods[c] = {}\n",
        "            for i in range(train_features.shape[1]):\n",
        "                feature_values = train_features[train_classes == c, i]\n",
        "                self.likelihoods[c][i] = dict(Counter(feature_values))\n",
        "                for value in self.likelihoods[c][i]:\n",
        "                    self.likelihoods[c][i][value] /= class_counts[c]\n",
        "\n",
        "    def data_discretization(self, data):\n",
        "        for i in range(data.shape[1]):\n",
        "            bin_size = 5\n",
        "            data_min = min(data[:,i])\n",
        "            data_max = max(data[:,i])\n",
        "            bins = np.linspace(data_min, data_max, bin_size)\n",
        "            data[:,i] = np.digitize(data[:,i], bins, right=True)\n",
        "        return data\n",
        "\n",
        "    def predict(self, sample):\n",
        "        posteriors = {}\n",
        "        for c in self.priors:\n",
        "            posteriors[c] = self.priors[c]\n",
        "            for i in range(sample.shape[0]):\n",
        "                feature_value = sample[i]\n",
        "                if feature_value in self.likelihoods[c][i]:\n",
        "                    posteriors[c] *= self.likelihoods[c][i][feature_value]\n",
        "                else:\n",
        "                    posteriors[c] *= 0\n",
        "        return max(posteriors, key=posteriors.get)\n",
        "    \n",
        "    def _test(self, x, y, test_size):\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
        "        self.build_classifier(x_train, y_train)\n",
        "        predictions = []\n",
        "        x_test = self.data_discretization(x_test)\n",
        "        for sample in x_test:\n",
        "            predictions.append(self.predict(sample))\n",
        "        return sum(predictions == y_test) / len(y_test)\n",
        "\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        self.classes = list(set(train_classes))\n",
        "        for c in self.classes:\n",
        "            self.priors[c] = Counter(train_classes)[c] / len(train_classes)\n",
        "            self.likelihoods[c] = []\n",
        "            for i in range(train_features.shape[1]):\n",
        "                feature_values = train_features[train_classes==c, i]\n",
        "                mean = np.mean(feature_values)\n",
        "                std = np.std(feature_values)\n",
        "                self.likelihoods[c].append((mean, std))\n",
        "\n",
        "    def normal_dist(self, x, mean, std):\n",
        "        return (1 / (std * math.sqrt(2 * math.pi))) * math.exp(-((x - mean) ** 2) / (2 * std ** 2))\n",
        "\n",
        "    def predict(self, test):\n",
        "        pred = []\n",
        "        for sample in test:\n",
        "            predictions = {}\n",
        "            for c in self.classes:\n",
        "                predictions[c] = self.priors[c]\n",
        "                for i in range(sample.shape[0]):\n",
        "                    predictions[c] *= self.normal_dist(sample[i], self.likelihoods[c][i][0], self.likelihoods[c][i][1])\n",
        "            pred.append(max(predictions, key=predictions.get))\n",
        "        return pred\n",
        "\n",
        "    def _test(self, x, y, test_size):\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
        "        self.build_classifier(x_train, y_train)\n",
        "        predictions = self.predict(x_test)\n",
        "        return sum(predictions == y_test) / len(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "gnb = GaussianNaiveBayes()\n",
        "print(gnb._test(x, y, 0.1))\n",
        "nb = NaiveBayes()\n",
        "print(nb._test(x, y, 0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_size:  0.1\n",
            "Naive -- mean accuracy: 83.63333333333338 %\n",
            "Naive Gauss -- mean accuracy: 95.46666666666677 %\n",
            "test_size:  0.2\n",
            "Naive -- mean accuracy: 86.48333333333343 %\n",
            "Naive Gauss -- mean accuracy: 95.16666666666683 %\n",
            "test_size:  0.3\n",
            "Naive -- mean accuracy: 86.6666666666666 %\n",
            "Naive Gauss -- mean accuracy: 95.32222222222211 %\n",
            "test_size:  0.4\n",
            "Naive -- mean accuracy: 86.8583333333334 %\n",
            "Naive Gauss -- mean accuracy: 95.18333333333327 %\n",
            "test_size:  0.5\n",
            "Naive -- mean accuracy: 87.23333333333333 %\n",
            "Naive Gauss -- mean accuracy: 95.27333333333335 %\n"
          ]
        }
      ],
      "source": [
        "nb = NaiveBayes()\n",
        "gnb = GaussianNaiveBayes()\n",
        "nb_mean = [0, 0, 0, 0, 0]\n",
        "gnb_mean = [0, 0, 0, 0, 0]\n",
        "tests_size = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "samples = 1000\n",
        "for i in range(samples):\n",
        "    nb_mean[i%len(tests_size)] += nb._test(x, y, tests_size[i%len(tests_size)])\n",
        "    gnb_mean[i%len(tests_size)] += gnb._test(x, y, tests_size[i%len(tests_size)])\n",
        "for i in range(len(tests_size)):\n",
        "    print(\"test_size: \", tests_size[i])\n",
        "    print(\"Naive -- mean accuracy:\", nb_mean[i] *len(tests_size)/samples * 100, \"%\")\n",
        "    print(\"Naive Gauss -- mean accuracy:\", gnb_mean[i]*len(tests_size)/samples*100, \"%\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wnioski:\n",
        "dla różnych rozmiarów testów Gauss średnio działa zawsze lepiej.\n",
        "Średnia dla gausa wynosi około 95% skuteczność, zaś dla naiwnego wynosi ok 86%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "d82ac94077643f57bd8e27ef7c0092ba99f1c8d5d58dd1340bf8629343aa5cca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
